---
tags:
  - math_220
created: 2024-12-03
see also:
- "[[6.3 Orthogonal Projections]]"
---

> *Definition: Least-Squares Solution*
> If $A$ is $m \times n$ and $\vec b$ is in $\mathbb R^m$, a **least-squares solution** of $A \vec x = \vec b$ is an $\vec{\hat x}$ in $\mathbb R^n$ such that
> $$ ||\vec b - A \vec{\hat x}|| \leq ||\vec b - A \vec x||$$
> for all $\vec x$ in $\mathbb R^n$.

> *Theorem*
> The set of least-squares solutions of $A \vec x = \vec b$ coincides with the nonempty set of solutions of the normal equations $A^T A \vec x = A^T \vec b$.

> *Theorem*
> Let $A$ be an $m \times n$ matrix. The following statements are logically equivalent:
> 1. The equation $A \vec x = \vec b$ has a unique least-squares solution for each $\vec b$ in $\mathbb R^m$.
> 2. The columns of $A$ are [[1.7 Linear Independence|linearly independent]].
> 3. The matrix $A^T A$ is [[2.2 Inverses|invertible]].

> *Theorem*
> Given an $m \times n$ matrix $A$ with linearly independent columns, let $A = QR$ be a $QR$ [[6.4 The Gram-Schmidt Process|factorisation]] of $A$. The, for each $\vec b$ in $\mathbb R^m$, the equations $A \vec x = \vec b$ has a unique least-squares solution, given by
> $$ \vec{\hat x} = R^{-1} Q^T \vec b. $$