---
tags:
  - math_220
created: 2024-09-10
---

> *Definition: Linear Independence vs. Linear Dependence*
> An indexed set of vectors $\{ \vec v_1, \dots, \vec v_p \}$ is said to be **linearly independent** if the vector equation
> $$ x_1 vec v_1 + x_2 \vec v_2 + \dots + x_p \vec v_p = \vec 0 $$
> has only the trivial solution (the zero vector). The set $\{ \vec v_1, \dots, \vec v_p \}$ is said to be **linearly dependent** if there exists weights $c_1, c_2, \dots, c_p$, not all zero, such that
> $$ c_1 \vec v_1 + c_2 \vec v_2 + \dots + c_p \vec v_p = \vec 0. $$

Note: The above equation is called a **linear dependence relation** among $\vec v_1, \dots, \vec v_p$ when the weights are not all zero.

---

> *Example*
> Determine if the set of $\{ \vec v_1, \vec v_2, \vec v_3 \}$ is linearly independent given that
> $$ v_1 = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}, v_2 = \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix}, \text{ and } v_3 = \begin{bmatrix} 2 \\ 1 \\ 0 \end{bmatrix}. $$

Set up equation:

$\begin{bmatrix} 1 & 4 & 2 & | & 0 \\ 2 & 5 & 1 & | & 0 \\ 3 & 6 & 0 & | & 0 \end{bmatrix}$

After row reductions, we end up with

$\begin{bmatrix} 1 & 0 & -2 & | & 0 \\ 0 & 1 & 1 & | & 0 \\ 0 & 0 & 0 & | & 0 \end{bmatrix}$

Since the last row is a free variable, there are infinite solutions. Therefore, the set of $\{ \vec v_1, \vec v_2, \vec v_3 \}$ is *linearly dependent*.

> *Example*
> Determine if the columns of matrix $A = \begin{bmatrix} 0 & 1 & 4 \\ 1 & 2 & -1 \\ 5 & 8 & 0 \end{bmatrix}$ are linearly independent.

After row reductions:

$\begin{bmatrix} 1 & 2 & -1 & | & 0 \\ 0 & 1 & 4 & | & 0 \\ 0 & 0 & 13 & | & 0 \end{bmatrix}$

As soon as we see a solution, we know the columns are *linearly independent*. Observe that the only single solution possible is $x_1 = x_2 = x_3 = 0$.
By this logic, as soon as we see a free variable, we know the system is linearly dependent.

> *Question*
> Is the set of $\{ \vec v_1 \}$ linearly independent?

We need $x_1 \vec v_1 = \vec 0$
- If $\vec v_1 = \vec 0$, then the system is linearly dependent (any $x_1$ will satisfy the equation).
- If $\vec v_1 \neq \vec 0$, then the system is linearly independent.

> *Question*
> Let $\vec u$ and $\vec v$ be linearly independent vectors.
> If a vector $\vec w$ is in $\text{Span} \{ \vec u, \vec v, \vec w \}$, is the set of vectors $\{ \vec u, vec v, \vec w \}$ linearly independent?

No, because there are $c_1, c_2$ that satisfy $c_1 \vec u + c_2 \vec v = \vec w$.

> *Question*
> If $\{ \vec u, \vec v, \vec w \}$ is linearly dependent, is $w$ in $\text{Span} \{ \vec u, \vec v \}$?

Yes, using the above logic.

---

> *Theorem*
> An indexed set $S = \{ \vec v_1, \dots, \vec v_p \}$ of two or more vectors is linearly dependent if and only if at least one of the vectors in $S$ is a linear combination of the others. In fact, if $S$ is linearly dependent and $\vec v_1 \neq 0$, then some $\vec v_j$ (with $j > 1$) is a linear combination of the preceding vectors $\vec v_1, \dots, \vec v_{j - 1}$.

Note: If there is a zero vector into the set, it is impossible for the set to be linearly independent. In other words, $\vec 0$ implies linear dependence.

> *Theorem*
> If a set $S = \{ \vec v_1, \dots, \vec v_p \}$ in $\mathbb R^n$ contains the zero vector, then the set is linearly dependent.

> *Theorem*
> If a set contains more vectors than there are entries in each vector, then the set is linearly dependent. That is, any set $\{ \vec v_1, \dots, \vec v_p \}$ in $\mathbb R^n$ is linearly dependent if $p > n$.

Also known as the [pigeonhole principle](https://en.wikipedia.org/wiki/Pigeonhole_principle).